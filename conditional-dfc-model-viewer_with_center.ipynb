{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import dlib\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import MorphII_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_morphii_stats(stats_csv):\n",
    "    \"\"\"\n",
    "    Loads the precomputed Morph-II statistics.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(stats_csv)\n",
    "    mean_brightness = float(df[\"mean_brightness\"][0])\n",
    "    histogram_str = df[\"histogram\"][0].strip().strip('\"')\n",
    "    mean_histogram = np.array(ast.literal_eval(histogram_str))\n",
    "    return mean_brightness, mean_histogram\n",
    "\n",
    "def adjust_brightness(image, target_brightness):\n",
    "    \"\"\"\n",
    "    Adjusts the image brightness.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    current_brightness = np.mean(gray)\n",
    "    factor = target_brightness / current_brightness\n",
    "    image = np.clip(image * factor, 0, 255).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "def normalize_background(image, background_value=128):\n",
    "    \"\"\"\n",
    "    Normalizes bright backgrounds to a uniform grey.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    mask = gray > 200  # assume bright background\n",
    "    image[mask] = background_value\n",
    "    return image\n",
    "\n",
    "def manual_match_histogram(source, target_hist):\n",
    "    \"\"\"\n",
    "    Matches the histogram of a grayscale source image to a target histogram.\n",
    "    Implements a simple cumulative distribution mapping.\n",
    "    \"\"\"\n",
    "    # Compute source histogram and CDF\n",
    "    source_hist, _ = np.histogram(source.flatten(), bins=256, range=(0,256), density=True)\n",
    "    source_cdf = np.cumsum(source_hist)\n",
    "    target_cdf = np.cumsum(target_hist)\n",
    "    # Create mapping from source to target intensities\n",
    "    mapping = np.interp(source_cdf, target_cdf, np.arange(256))\n",
    "    matched = mapping[source]\n",
    "    return matched.astype(np.uint8)\n",
    "\n",
    "def match_histogram(image, target_hist):\n",
    "    \"\"\"\n",
    "    Matches the histogram of the L channel in LAB space to the target histogram,\n",
    "    then converts back to BGR to preserve the original color information.\n",
    "    \"\"\"\n",
    "    # Convert from BGR to LAB\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    L, A, B = cv2.split(lab)\n",
    "\n",
    "    # Apply manual histogram matching on the L channel only.\n",
    "    matched_L = manual_match_histogram(L, target_hist)\n",
    "\n",
    "    # Merge the adjusted L channel back with the original A and B channels.\n",
    "    matched_lab = cv2.merge([matched_L, A, B])\n",
    "    matched_bgr = cv2.cvtColor(matched_lab, cv2.COLOR_LAB2BGR)\n",
    "    return matched_bgr\n",
    "\n",
    "def preprocess_image(image, stats_csv):\n",
    "    \"\"\"\n",
    "    Applies brightness adjustment, background normalization, and histogram matching.\n",
    "    \"\"\"\n",
    "    mean_brightness, mean_histogram = load_morphii_stats(stats_csv)\n",
    "    image = adjust_brightness(image, mean_brightness)\n",
    "    image = normalize_background(image)\n",
    "    image = match_histogram(image, mean_histogram)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def align_face(image, stats_csv):\n",
    "    \"\"\"\n",
    "    Aligns the face in the input image and applies dataset normalization.\n",
    "    Expects image in BGR format.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray, 1)\n",
    "    if len(faces) == 0:\n",
    "        raise ValueError(\"No face detected in the image.\")\n",
    "    face = faces[0]\n",
    "    landmarks = predictor(gray, face)\n",
    "    landmarks = [(p.x, p.y) for p in landmarks.parts()]\n",
    "\n",
    "    # Compute eye centers.\n",
    "    left_eye_center = np.mean(landmarks[36:42], axis=0).astype(\"int\")\n",
    "    right_eye_center = np.mean(landmarks[42:48], axis=0).astype(\"int\")\n",
    "\n",
    "    # Calculate rotation angle and scaling.\n",
    "    dY = right_eye_center[1] - left_eye_center[1]\n",
    "    dX = right_eye_center[0] - left_eye_center[0]\n",
    "    angle = np.degrees(np.arctan2(dY, dX))\n",
    "    desired_right_eye_x = 1.0 - 0.35\n",
    "    dist = np.sqrt(dX**2 + dY**2)\n",
    "    desired_dist = (desired_right_eye_x - 0.35) * 256\n",
    "    scale = desired_dist / dist\n",
    "    eyes_center = ((left_eye_center[0] + right_eye_center[0]) / 2,\n",
    "                   (left_eye_center[1] + right_eye_center[1]) / 2)\n",
    "\n",
    "    # Compute affine transform.\n",
    "    M = cv2.getRotationMatrix2D(eyes_center, angle, scale)\n",
    "    tX = 256 * 0.5\n",
    "    tY = 256 * 0.35\n",
    "    M[0, 2] += (tX - eyes_center[0])\n",
    "    M[1, 2] += (tY - eyes_center[1])\n",
    "\n",
    "    aligned_face = cv2.warpAffine(image, M, (256, 256), flags=cv2.INTER_CUBIC)\n",
    "    processed_face = preprocess_image(aligned_face, stats_csv)\n",
    "    return processed_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_csv = \"morphii_train_stats.csv\"\n",
    "prepipeline = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Lambda(lambda img: cv2.cvtColor(\n",
    "        align_face(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR), stats_csv),\n",
    "        cv2.COLOR_BGR2RGB\n",
    "    )),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"Dataset/Team pics/Kyler.jpeg\"\n",
    "bgr_img = cv2.imread(img_path)\n",
    "if bgr_img is None:\n",
    "    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "# Convert to RGB as expected by the pipeline.\n",
    "rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "processed_img = prepipeline(rgb_img)\n",
    "\n",
    "plt.imshow(processed_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Processed (Face-Aligned) Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MorphII_Dataset(csv_file=\"Dataset/Team pics/team.csv\", transform=prepipeline)\n",
    "test_dataset = MorphII_Dataset(csv_file=\"Dataset/Team pics/team.csv\", transform=prepipeline)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder: maps image and condition -> latent mean and logvar.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=100, condition_dim=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),   # B x 3 x 64 x 64 -> B x 16 x 32 x 32\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # B x 16 x 32 x 32 -> B x 32 x 16 x 16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # B x 32 x 16 x 16 -> B x 64 x 8 x 8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # B x 64 x 8 x 8 -> B x 128 x 4 x 4\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        # 128 x 4 x 4 = 2048\n",
    "        self.fc_mu = nn.Linear(2048 + condition_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(2048 + condition_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv(x)              # shape: (B, 128, 4, 4)\n",
    "        x = x.view(batch_size, -1)    # flatten to (B, 2048)\n",
    "        x = torch.cat([x, condition], dim=1)  # concatenate condition (B, 2048+condition_dim)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=100, condition_dim=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + condition_dim, 2048)   # B x (latent_dim+condition_dim) -> B x 2048\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2,\n",
    "                               padding=1, output_padding=1),    # B x 128 x 4 x 4 -> B x 64 x 8 x 8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2,\n",
    "                               padding=1, output_padding=1),    # B x 64 x 8 x 8 -> B x 32 x 16 x 16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2,\n",
    "                               padding=1, output_padding=1),    # B x 32 x 16 x 16 -> B x 16 x 32 x 32\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2,\n",
    "                               padding=1, output_padding=1),    # B x 16 x 32 x 32 -> B x 3 x 64 x 64\n",
    "            nn.Tanh()  # Output in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z, condition):\n",
    "        x = torch.cat([z, condition], dim=1)  # shape: (B, latent_dim+condition_dim)\n",
    "        x = self.fc(x)                        # (B, 2048)\n",
    "        x = x.view(-1, 128, 4, 4)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=100, condition_dim=1):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, condition_dim)\n",
    "        self.decoder = Decoder(latent_dim, condition_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        mu, logvar = self.encoder(x, condition)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        recon_x = self.decoder(z, condition)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "latent_dim = 100\n",
    "condition_dim = 1  # only using age, we could expand this to gender and race\n",
    "model = ConditionalVAE(latent_dim, condition_dim).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "except Exception as e:\n",
    "    print(\"Channels last format not supported:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/checkpoint_epoch_500.pth\"\n",
    "load_checkpoint(model, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        img, cond = val_dataset[i]\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        cond = cond.unsqueeze(0).to(device)\n",
    "        recon, _, _ = model(img, cond)\n",
    "\n",
    "        orig_np = (img.squeeze().cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        recon_np = (recon.squeeze().cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(orig_np)\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(recon_np)\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_age_variation(model, image, cond, age_values):\n",
    "    \"\"\"\n",
    "    Given an image and its condition, encode it and then decode it\n",
    "    with varying age conditions.\n",
    "\n",
    "    Args:\n",
    "        model: Trained ConditionalVAE.\n",
    "        image: A single image tensor (C x H x W).\n",
    "        cond: Its corresponding condition tensor (age), shape [1].\n",
    "        age_values: Iterable of new normalized age values.\n",
    "\n",
    "    Returns:\n",
    "        List of generated images (tensors).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        cond = cond.unsqueeze(0).to(device)\n",
    "\n",
    "        mu, logvar = model.encoder(image, cond)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        for age in age_values:\n",
    "            new_cond = torch.tensor([[age]], dtype=torch.float32).to(device)\n",
    "            out = model.decoder(z, new_cond)\n",
    "            outputs.append(out)\n",
    "    return outputs\n",
    "\n",
    "sample_img, sample_cond = test_dataset[3]\n",
    "age_range = np.linspace(0.0, 1.0, 10)\n",
    "generated_images = generate_age_variation(model, sample_img, sample_cond, age_range)\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, gen in enumerate(generated_images):\n",
    "    gen_np = (gen.squeeze().cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "    plt.subplot(1, len(generated_images), i+1)\n",
    "    plt.imshow(gen_np)\n",
    "    plt.title(f\"Age: {age_range[i]:.2f}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
