{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import dlib\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import MorphII_Dataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_NUMBER = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepipeline = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_dataset = MorphII_Dataset(csv_file=\"Dataset/Index/validation.csv\", transform=prepipeline)\n",
    "test_dataset = MorphII_Dataset(csv_file=\"Dataset/Index/test.csv\", transform=prepipeline)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=100, condition_dim=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # 128×128 -> 64×64\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # 64×64 -> 32×32\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # 32×32 -> 16×16\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # 16×16 -> 8×8\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        # 128 x 8 x 8 = 8192 features\n",
    "        self.fc_mu = nn.Linear(8192 + condition_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(8192 + condition_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv(x)  # shape: (B, 128, 8, 8)\n",
    "        x = x.view(batch_size, -1)  # flatten to (B, 8192)\n",
    "        x = torch.cat([x, condition], dim=1)  # shape: (B, 8192+condition_dim)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=100, condition_dim=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + condition_dim, 8192)\n",
    "        self.deconv = nn.Sequential(\n",
    "            # Reshape (B, 128, 8, 8) -> upsample to 16×16\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2,\n",
    "                               padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # 16×16 -> 32×32\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2,\n",
    "                               padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # 32×32 -> 64×64\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2,\n",
    "                               padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # 64×64 -> 128×128\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2,\n",
    "                               padding=1, output_padding=1),\n",
    "            nn.Tanh()  # Output in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z, condition):\n",
    "        x = torch.cat([z, condition], dim=1)  # shape: (B, latent_dim+condition_dim)\n",
    "        x = self.fc(x)                        # (B, 8192)\n",
    "        x = x.view(-1, 128, 8, 8)              # reshape to (B, 128, 8, 8)\n",
    "        x = self.deconv(x)                    # output: (B, 3, 128, 128)\n",
    "        return x\n",
    "\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=100, condition_dim=1):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, condition_dim)\n",
    "        self.decoder = Decoder(latent_dim, condition_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        mu, logvar = self.encoder(x, condition)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        recon_x = self.decoder(z, condition)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "latent_dim = 256\n",
    "condition_dim = 2\n",
    "model = ConditionalVAE(latent_dim, condition_dim).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "except Exception as e:\n",
    "    print(\"Channels last format not supported:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "\n",
    "checkpoint_path = f\"checkpoints/checkpoint_epoch_{CHECKPOINT_NUMBER}.pth\"\n",
    "load_checkpoint(model, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_displayed = 5\n",
    "\n",
    "fig, axs = plt.subplots(num_displayed, 2, figsize=(6, 3 * num_displayed))\n",
    "\n",
    "for idx in range(num_displayed):\n",
    "    sample_img, sample_cond = test_dataset[idx]\n",
    "    img_tensor = sample_img.unsqueeze(0).to(device)\n",
    "    cond_tensor = sample_cond.unsqueeze(0).to(device)\n",
    "    recon, _, _ = model(img_tensor, cond_tensor)\n",
    "\n",
    "    orig_np = (img_tensor.squeeze().cpu().detach().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "    recon_np = (recon.squeeze().cpu().detach().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "\n",
    "    axs[idx, 0].imshow(orig_np)\n",
    "    axs[idx, 0].set_title(\"Original\")\n",
    "    axs[idx, 0].axis(\"off\")\n",
    "\n",
    "    axs[idx, 1].imshow(recon_np)\n",
    "    axs[idx, 1].set_title(\"Reconstructed\")\n",
    "    axs[idx, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figures/test_reconstruction.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_age_variation(model, image, cond, age_values):\n",
    "    \"\"\"\n",
    "    Given an image and its condition, encode it and then decode it\n",
    "    with varying age conditions. The condition now is assumed to contain\n",
    "    [normalized_age, gender] and only the age is varied.\n",
    "\n",
    "    Args:\n",
    "        model: Trained ConditionalVAE.\n",
    "        image: A single image tensor (C x H x W).\n",
    "        cond: Its corresponding condition tensor (age, gender), shape [2].\n",
    "        age_values: Iterable of new normalized age values.\n",
    "\n",
    "    Returns:\n",
    "        List of generated images (tensors).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        cond = cond.unsqueeze(0).to(device)\n",
    "\n",
    "        mu, logvar = model.encoder(image, cond)\n",
    "        z = reparameterize(mu, logvar)\n",
    "\n",
    "        orig_gender = cond[0, 1].item()\n",
    "        for age in age_values:\n",
    "            new_cond = torch.tensor([[age, orig_gender]], dtype=torch.float32, device=device)\n",
    "            out = model.decoder(z, new_cond)\n",
    "            outputs.append(out)\n",
    "    return outputs\n",
    "\n",
    "def generate_gender_variation(model, image, cond):\n",
    "    \"\"\"\n",
    "    Given an image and its condition, encode it and then decode it\n",
    "    with varying gender conditions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        cond = cond.unsqueeze(0).to(device)\n",
    "\n",
    "        mu, logvar = model.encoder(image, cond)\n",
    "        z = reparameterize(mu, logvar)\n",
    "\n",
    "        orig_age = cond[0, 0].item()\n",
    "        for gender in [0, 1]:\n",
    "            new_cond = torch.tensor([[orig_age, gender]], dtype=torch.float32, device=device)\n",
    "            out = model.decoder(z, new_cond)\n",
    "            outputs.append(out)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_range = np.linspace(0, 1.5, 10)\n",
    "normalizer = 1.5\n",
    "\n",
    "fig, axs = plt.subplots(num_displayed, len(age_range), figsize=(2*len(age_range), 2*num_displayed))\n",
    "\n",
    "for row in range(num_displayed):\n",
    "    sample_img, sample_cond = test_dataset[row]\n",
    "    age_range = np.linspace(0.0, normalizer, 10)\n",
    "    generated_age_images = generate_age_variation(model, sample_img, sample_cond, age_range)\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for col, gen in enumerate(generated_age_images):\n",
    "        gen_np = (gen.squeeze().cpu().detach().numpy().transpose(1,2,0) * 0.5 + 0.5)\n",
    "        axs[row, col].imshow(gen_np)\n",
    "        axs[row, col].set_title(f\"Age: {int(16 + (1/normalizer)*age_range[col]*(80-16))}\")\n",
    "        axs[row, col].axis(\"off\")\n",
    "\n",
    "plt.savefig(\"figures/test_age.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img, sample_cond = test_dataset[0]\n",
    "generated_gender_images = generate_gender_variation(model, sample_img, sample_cond)\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, gen in enumerate(generated_gender_images):\n",
    "    gen_np = (gen.squeeze().cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "    plt.subplot(1, len(generated_gender_images), i+1)\n",
    "    plt.imshow(gen_np)\n",
    "    plt.title(f\"Gender: {[0, 1][i]:.2f}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_morphii_stats(stats_csv):\n",
    "    df = pd.read_csv(stats_csv)\n",
    "    mean_brightness = float(df[\"mean_brightness\"][0])\n",
    "    histogram_str = df[\"histogram\"][0].strip().strip('\"')\n",
    "    mean_histogram = np.array(ast.literal_eval(histogram_str))\n",
    "    return mean_brightness, mean_histogram\n",
    "\n",
    "def adjust_brightness_masked(image, target_brightness, mask, custom_brightness_constant = 1):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if np.sum(mask)==0:\n",
    "        return image\n",
    "    current_brightness = np.mean(gray[mask])\n",
    "    factor = target_brightness / current_brightness * custom_brightness_constant\n",
    "    image_adj = image.copy().astype(np.float32)\n",
    "    image_adj[mask] = image_adj[mask] * factor\n",
    "    image_adj = np.clip(image_adj, 0, 255).astype(np.uint8)\n",
    "    return image_adj\n",
    "\n",
    "def manual_match_histogram(source, target_hist):\n",
    "    # Standard manual matching on a 1D array.\n",
    "    hist, _ = np.histogram(source.flatten(), bins=256, range=(0,256), density=True)\n",
    "    cdf_source = np.cumsum(hist)\n",
    "    cdf_target = np.cumsum(target_hist)\n",
    "    mapping = np.interp(cdf_source, cdf_target, np.arange(256))\n",
    "    matched = mapping[source]\n",
    "    return matched.astype(np.uint8)\n",
    "\n",
    "def match_histogram_masked(image, target_hist, mask):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    L, A, B = cv2.split(lab)\n",
    "    L_matched = L.copy()\n",
    "    if np.sum(mask) > 0:\n",
    "        L_fg = L[mask]\n",
    "        matched_fg = manual_match_histogram(L_fg, target_hist)\n",
    "        L_matched[mask] = matched_fg\n",
    "    matched_lab = cv2.merge([L_matched, A, B])\n",
    "    matched_bgr = cv2.cvtColor(matched_lab, cv2.COLOR_LAB2BGR)\n",
    "    return matched_bgr\n",
    "\n",
    "def preprocess_image_custom_masked(image, stats_csv, mask, custom_brightness_constant = 1):\n",
    "    mean_brightness, mean_histogram = load_morphii_stats(stats_csv)\n",
    "    image_adj = adjust_brightness_masked(image, mean_brightness, mask, custom_brightness_constant)\n",
    "    image_matched = match_histogram_masked(image_adj, mean_histogram, mask)\n",
    "    return image_matched\n",
    "\n",
    "def estimate_background_color_top(image, height=10, width=2):\n",
    "    \"\"\"\n",
    "    Estimates the background color by sampling a vertical slice from the top\n",
    "    left and right corners of the image, and then taking the mode of those pixels.\n",
    "    \"\"\"\n",
    "    constant=0\n",
    "    h, w, _ = image.shape\n",
    "    # Grab a 2x10 patch from top left and top right.\n",
    "    patch_left = image[constant:height+constant, 0:width, :]        # shape (height, width, 3)\n",
    "    patch_right = image[constant:height+constant, w-width:w, :]       # shape (height, width, 3)\n",
    "\n",
    "    combined = np.concatenate([patch_left.reshape(-1, 3),\n",
    "                               patch_right.reshape(-1, 3)], axis=0)\n",
    "    # Convert each pixel to a tuple and compute the mode.\n",
    "    pixel_list = [tuple(pixel) for pixel in combined]\n",
    "    mode_color = Counter(pixel_list).most_common(1)[0][0]\n",
    "    return np.array(mode_color, dtype=np.uint8)\n",
    "\n",
    "def remove_background(image, tolerance=30, height=10, width=2):\n",
    "    \"\"\"\n",
    "    Computes a gentle background mask based on the difference from a background\n",
    "    color estimated from the top corner slices using the mode.\n",
    "    Pixels that differ from the estimated color by more than 'tolerance'\n",
    "    (Euclidean distance) are considered foreground.\n",
    "    \"\"\"\n",
    "    bg_color = estimate_background_color_top(image, height, width)\n",
    "    diff = np.linalg.norm(image.astype(np.float32) - bg_color.astype(np.float32), axis=2)\n",
    "    mask = diff > tolerance\n",
    "\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=6)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=6)\n",
    "    return mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_face(image, stats_csv, custom_brightness_constant=1):\n",
    "    \"\"\"\n",
    "    Aligns the face in the input image and applies masked brightness and histogram adjustments\n",
    "    only on the foreground. Then it replaces the background with a constant grey.\n",
    "    Expects the input image in BGR format.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray, 1)\n",
    "    if len(faces) == 0:\n",
    "        raise ValueError(\"No face detected in the image.\")\n",
    "    face = faces[0]\n",
    "    landmarks = predictor(gray, face)\n",
    "    landmarks = [(p.x, p.y) for p in landmarks.parts()]\n",
    "\n",
    "    left_eye = np.mean(landmarks[36:42], axis=0).astype(int)\n",
    "    right_eye = np.mean(landmarks[42:48], axis=0).astype(int)\n",
    "    dY = right_eye[1] - left_eye[1]\n",
    "    dX = right_eye[0] - left_eye[0]\n",
    "    angle = np.degrees(np.arctan2(dY, dX))\n",
    "    desired_right_eye_x = 1.0 - 0.35\n",
    "    dist = np.sqrt(dX**2 + dY**2)\n",
    "    desired_dist = (desired_right_eye_x - 0.35) * 256\n",
    "    scale = desired_dist / dist\n",
    "\n",
    "    eyes_center = (float((left_eye[0] + right_eye[0]) / 2),\n",
    "                   float((left_eye[1] + right_eye[1]) / 2))\n",
    "    M = cv2.getRotationMatrix2D(eyes_center, angle, scale)\n",
    "    tX, tY = 256 * 0.5, 256 * 0.35\n",
    "    M[0, 2] += (tX - eyes_center[0])\n",
    "    M[1, 2] += (tY - eyes_center[1])\n",
    "    grey_bg = (128, 128, 128)\n",
    "    aligned_face = cv2.warpAffine(image, M, (256, 256), flags=cv2.INTER_CUBIC,\n",
    "                                  borderMode=cv2.BORDER_CONSTANT, borderValue=tuple(grey_bg))\n",
    "\n",
    "    mask = remove_background(aligned_face, tolerance=30)\n",
    "    processed_fg = preprocess_image_custom_masked(aligned_face, stats_csv, mask, custom_brightness_constant)\n",
    "    final_img = processed_fg.copy()\n",
    "\n",
    "    final_img[~mask] = np.array([128, 128, 128], dtype=np.uint8)\n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "stats_csv = \"morphii_train_stats.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepipeline_custom = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Lambda(lambda img: cv2.cvtColor(\n",
    "        align_face(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR), stats_csv),\n",
    "        cv2.COLOR_BGR2RGB\n",
    "    )),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"Dataset/Team pics/Brad.jpeg\"\n",
    "bgr_img = cv2.imread(img_path)\n",
    "if bgr_img is None:\n",
    "    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "processed_img = prepipeline_custom(rgb_img)\n",
    "\n",
    "plt.imshow(processed_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Processed (Face-Aligned) Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = MorphII_Dataset(csv_file=\"Dataset/Team pics/team.csv\", transform=prepipeline_custom)\n",
    "custom_loader = DataLoader(custom_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3*5))\n",
    "for idx in range(5):\n",
    "    custom_img, custom_cond = custom_dataset[idx]\n",
    "    img_tensor = custom_img.unsqueeze(0).to(device)\n",
    "    cond_tensor = custom_cond.unsqueeze(0).to(device)\n",
    "    recon, _, _ = model(img_tensor, cond_tensor)\n",
    "\n",
    "    orig_np = (img_tensor.squeeze().cpu().detach().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "    recon_np = (recon.squeeze().cpu().detach().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "\n",
    "    plt.subplot(5, 2, idx * 2 + 1)\n",
    "    plt.imshow(orig_np)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(5, 2, idx * 2 + 2)\n",
    "    plt.imshow(recon_np)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figures/team_reconstruction.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_custom = 5\n",
    "num_age = 5\n",
    "\n",
    "age_range = np.linspace(0, normalizer, num_age)\n",
    "\n",
    "fig, axs = plt.subplots(num_custom, num_age, figsize=(3*num_age, 3*num_custom))\n",
    "\n",
    "for row in range(num_custom):\n",
    "    custom_img, custom_cond = custom_dataset[row]\n",
    "    generated_age_images = generate_age_variation(model, custom_img, custom_cond, age_range)\n",
    "\n",
    "    for col, gen in enumerate(generated_age_images):\n",
    "        gen_np = (gen.squeeze().cpu().detach().numpy().transpose(1,2,0) * 0.5 + 0.5)\n",
    "        axs[row, col].imshow(gen_np)\n",
    "        axs[row, col].set_title(f\"Age: {int(16 + (1/normalizer)*age_range[col]*(80-16))}\")\n",
    "        axs[row, col].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figures/team_age.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image as IPyImage, display\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "name_dictionary = {\n",
    "    1: \"Kyler\",\n",
    "    2: \"Brad\",\n",
    "    3: \"John\",\n",
    "    4: \"Casey\",\n",
    "    5: \"Batu\"\n",
    "}\n",
    "\n",
    "num_frames = 100\n",
    "gif_filenames = []\n",
    "for idx in range(num_custom):\n",
    "    custom_img, custom_cond = custom_dataset[idx]\n",
    "    age_range_frames = np.linspace(0, 2, num_frames)\n",
    "    generated_age_images_frames = generate_age_variation(model, custom_img, custom_cond, age_range_frames)\n",
    "\n",
    "    frames = []\n",
    "    for gen in generated_age_images_frames:\n",
    "        gen_np = (gen.squeeze().cpu().detach().numpy().transpose(1,2,0) * 0.5 + 0.5)\n",
    "        frame = (gen_np * 255).astype(np.uint8)\n",
    "        frames.append(frame)\n",
    "\n",
    "    bounce_frames = frames + frames[-2:0:-1]\n",
    "\n",
    "    gif_filename = f\"figures/gifs/age_variation_{name_dictionary[idx+1]}.gif\"\n",
    "    imageio.mimsave(gif_filename, bounce_frames, duration=0.01)\n",
    "    gif_filenames.append(gif_filename)\n",
    "\n",
    "print(\"Generated GIFs:\")\n",
    "for gif_filename in gif_filenames:\n",
    "    display(IPyImage(filename=gif_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
